{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the RAG workflow in LangGraph : Retrieval AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import List, TypedDict\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Websites url by adding them to a vector DB\n",
    "urls = [\n",
    "    \"....\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\"\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#Prepare the RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an assistant for question-answering tasks.Use the following pieces of retrieved context to answer the question.\n",
    "if you don't know the answer, just say that you don't know.Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context} \n",
    "Answer:                                                                                                                                                                       \n",
    "                                          \n",
    "\"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "rag_chain = (\n",
    "    prompt | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "#Define the graph\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generationn: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "\n",
    "#Retrieve node\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retreive the documents\n",
    "    Args:\n",
    "        state(dict): The current graph state\n",
    "    Returns:\n",
    "        state(dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Retrieve---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    #Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "#Generate node\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state(dict):The current graph state\n",
    "    Returns:\n",
    "        state(dict): New key added to state, generation, that contains LLM generation    \n",
    "    \"\"\"\n",
    "    print(\"---Generate---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    #RAG Generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "#Define the workflow\n",
    "def create_workflow():\n",
    "    workflow = StateGraph(GraphState)\n",
    "\n",
    "    #Add nodes\n",
    "    workflow.add_node(\"retrieve\", retrieve)\n",
    "    workflow.add_node(\"generate\", generate)\n",
    "\n",
    "    #Add edges\n",
    "    workflow.add_edge(START,\"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    return workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "#Run the workflow\n",
    "async def run_workflow():\n",
    "    app = create_workflow()\n",
    "    config = {\n",
    "        \"configurable\": {\"thread_id\": \"1\"},\n",
    "        \"recursion_limit\": 50\n",
    "    }\n",
    "    inputs = {\"question\": f\"What are flat indexes?\"}\n",
    "\n",
    "    try:\n",
    "        async for event in app.astream(inputs, config=config, stream_mode=\"values\"):\n",
    "            if \"error\" in event:\n",
    "                print(f\"Error: {event['error']}\")\n",
    "                break\n",
    "            print(event)\n",
    "    except Exception as e:\n",
    "        print(f\"Workflow wxecution failed: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    asyncio.run(run_workflow())       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
